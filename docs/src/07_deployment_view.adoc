ifndef::imagesdir[:imagesdir: ../images]

[[section-deployment-view]]


== Deployment View

[role="arc42help"]
****
.Content
The deployment view describes:

 1. technical infrastructure used to execute your system, with infrastructure elements like geographical locations, environments, computers, processors, channels and net topologies as well as other infrastructure elements and

2. mapping of (software) building blocks to that infrastructure elements.

Often systems are executed in different environments, e.g. development environment, test environment, production environment. In such cases you should document all relevant environments.

Especially document a deployment view if your software is executed as distributed system with more than one computer, processor, server or container or when you design and construct your own hardware processors and chips.

From a software perspective it is sufficient to capture only those elements of an infrastructure that are needed to show a deployment of your building blocks. Hardware architects can go beyond that and describe an infrastructure to any level of detail they need to capture.

.Motivation
Software does not run without hardware.
This underlying infrastructure can and will influence a system and/or some
cross-cutting concepts. Therefore, there is a need to know the infrastructure.

.Form

Maybe a highest level deployment diagram is already contained in section 3.2. as
technical context with your own infrastructure as ONE black box. In this section one can
zoom into this black box using additional deployment diagrams:

* UML offers deployment diagrams to express that view. Use it, probably with nested diagrams,
when your infrastructure is more complex.
* When your (hardware) stakeholders prefer other kinds of diagrams rather than a deployment diagram, let them use any kind that is able to show nodes and channels of the infrastructure.


.Further Information

See https://docs.arc42.org/section-7/[Deployment View] in the arc42 documentation.

****

Our project is configured using GitHub actions so that every new github release publish triggers an attempt to deploy the application to a server.

This server is hosted on Azure and is configured to run the application using Docker containers. Only 2 ports are open to the public: The port to be used by the web application and the port to be used by the gateway service.
We have configured in github a set of secrets: DEPLOY_HOST (the IP address of the server), DEPLOY_USER (the user to connect to the server), DEPLOY_KEY (the SSH key to connect to the server) that allows us to deploy the application to the server using SSH.

This setup enables our team to achieve continuous deployment and delivery (CD) together with continuous integration (CI).

=== Infrastructure Level 1

[role="arc42help"]
****
Describe (usually in a combination of diagrams, tables, and text):

* distribution of a system to multiple locations, environments, computers, processors, .., as well as physical connections between them
* important justifications or motivations for this deployment structure
* quality and/or performance features of this infrastructure
* mapping of software artifacts to elements of this infrastructure

For multiple environments or alternative deployments please copy and adapt this section of arc42 for all relevant environments.
****

[plantuml,"Deployment view L1",png]
----
node "Client Side" {
  component WC as "Web Client"
}

node "Server Side"{
  component GS as "Gateway Service"
  component WA as "Web Application"
}

node "Wikidata Server"{
  component WAPI as "Wikidata API"
}

node "Empathy LLM Server"{
  component EAPI as "Empathy API"
}

GS-[dashed]->WAPI
WA-[dashed]->GS
WC-[dashed]->WA
WA-[dashed]->WC
WC-[dashed]->GS
GS-[dashed]->EAPI

----





Motivation::

    * The diagram above illustrates the initial version of our architecture and the delineation among its components. Our approach adopts a straightforward client-server architecture, where the server interacts with external services such as Wikidata or Empathy LLM. This division enforces a clear separation between the client/frontend and the server/backend. Such separation benefits the entire system by ensuring that as long as the common API is implemented, the specific implementations can remain interchangeable.

    * Utilizing an Ubuntu server on Azure provides us with an isolated environment equipped with the essential configurations and installations necessary for running our services. By hosting our server on Azure, we can minimize costs associated with machine uptime while alleviating responsibilities such as security, availability, and maintenance.

    * We use Docker as it is a containerization platform that allows us to package our application and its dependencies into a standardized unit for software development. This approach ensures that our application will run consistently on any environment, regardless of the machineâ€™s configuration.

Quality and/or Performance Features::

    As mentioned earlier, the primary advantage of this architecture lies in the interchangeability of its components.


=== Infrastructure Level 2

[role="arc42help"]
****
Here you can see the internal structure of (some) infrastructure elements from level 1.
****

==== _Server Side_
[plantuml,"Deployment view L2",png]
----
node "Server Side"{
  component GS as "Gateway Service"
  component WA as "Web Application"
  component US as "User Service"
  component AS as "Authorization Service"
  component QS as "Question Service"
  component LLMS as "LLM Service"
  database MDB as "Mongo DB"
}




GS-[dashed]->WA
WA-[dashed]->GS

GS-[dashed]->AS
AS-[dashed]->GS

GS-[dashed]->US
US-[dashed]->GS

GS-[dashed]->QS
QS-[dashed]->GS

AS-[dashed]->MDB
MDB-[dashed]->AS

US-[dashed]->MDB
MDB-[dashed]->US

GS-[dashed]->LLMS
LLMS-[dashed]->GS

----
The gateway service works as an adapter between the micro services and the web client. The web application is the main interface for the user to interact with the system.

We've opted for a microservices architecture using Docker containers instead of a monolithic setup. Each service has its own Docker image. This Docker-based approach streamlines deployment, management, and resource utilization while enhancing the overall flexibility and scalability of our system.